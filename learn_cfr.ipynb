{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counterfactual Regret Minimization implementation\n",
    "#Rock Paper Scissors example\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "NUM_ACTIONS = 3\n",
    "NUM_PLAYERS = 2\n",
    "ROCK, PAPER, SCISSORS = range(NUM_ACTIONS)\n",
    "\n",
    "A = np.array(list(itertools.product(range(NUM_ACTIONS), repeat=NUM_PLAYERS))) #all combinations of simul actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, regrets_sum=np.zeros(NUM_ACTIONS)):\n",
    "        self._strategy = np.zeros(NUM_ACTIONS)\n",
    "        self.strategy_sum = np.zeros(NUM_ACTIONS)\n",
    "        self.regrets_sum = regrets_sum\n",
    "        self.number_of_updates = 0\n",
    "    \n",
    "    @property\n",
    "    def strategy(self):\n",
    "        self._strategy = np.maximum(self.regrets_sum, 0)\n",
    "        self._strategy = self._strategy/sum(self._strategy) if sum(self._strategy) > 0 \\\n",
    "                    else np.ones(NUM_ACTIONS)/NUM_ACTIONS\n",
    "        self.strategy_sum += self._strategy\n",
    "        return self._strategy\n",
    "    \n",
    "    def payoff(self, my_action, other_action):\n",
    "        mod_3 = (my_action - other_action) % 3\n",
    "        if mod_3 == 2:\n",
    "            return -1\n",
    "        else:\n",
    "            return mod_3\n",
    "    \n",
    "    def update_regrets(self, other_action, my_payoff):\n",
    "        self.regrets_sum += np.array([self.payoff(a, other_action) - my_payoff for a in range(NUM_ACTIONS)])\n",
    "    \n",
    "    def update_strategy(self, my_action, other_action):\n",
    "        my_payoff = self.payoff(my_action, other_action)\n",
    "        self.update_regrets(other_action, my_payoff)\n",
    "        self.number_of_updates += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mason/w/cs/fun/cfr/learn_cfr.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mason/w/cs/fun/cfr/learn_cfr.ipynb#ch0000002?line=1'>2</a>\u001b[0m me \u001b[39m=\u001b[39m Player(regrets_sum\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray([\u001b[39m400\u001b[39m, \u001b[39m150\u001b[39m, \u001b[39m0\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mason/w/cs/fun/cfr/learn_cfr.ipynb#ch0000002?line=2'>3</a>\u001b[0m opp \u001b[39m=\u001b[39m Player()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mason/w/cs/fun/cfr/learn_cfr.ipynb#ch0000002?line=3'>4</a>\u001b[0m my_optimal, opp_optimal \u001b[39m=\u001b[39m train(\u001b[39m100000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mason/w/cs/fun/cfr/learn_cfr.ipynb#ch0000002?line=4'>5</a>\u001b[0m my_optimal, opp_optimal\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "opp_strategy = np.array([.3, .3, .4])\n",
    "me = Player(regrets_sum=np.array([400, 150, 0]))\n",
    "opp = Player()\n",
    "my_optimal, opp_optimal = train(100000)\n",
    "my_optimal, opp_optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp.strategy_sum / opp.number_of_updates\n",
    "print(expectedUtility([1-.25-.6,.25,.6], mixed))\n",
    "expectedUtility(mixed, [1-.25-.6,.25,.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(strategy):\n",
    "    return np.random.choice(range(NUM_ACTIONS), p=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations):\n",
    "    for _ in range(iterations):\n",
    "        #⟨Get regret-matched mixed-strategy actions⟩\n",
    "        my_action = get_action(me.strategy)\n",
    "        opp_action = get_action(opp.strategy)\n",
    "        me.update_strategy(my_action, opp_action)\n",
    "        opp.update_strategy(opp_action, my_action)\n",
    "    \n",
    "    my_optimal = me.strategy_sum / me.number_of_updates\n",
    "    opp_optimal = opp.strategy_sum / opp.number_of_updates    \n",
    "    return my_optimal, opp_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random things i learned while cfr\n",
    "def getValue(c):\n",
    "    c1, c2 = c\n",
    "    if c1==c2:\n",
    "        return 0\n",
    "    elif c1==0 and c2==2:\n",
    "        return 1\n",
    "    elif c1==1 and c2==0:\n",
    "        return 1\n",
    "    elif c1==2 and c2==1:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "def payoffTable(p1v, p2v):\n",
    "    out = np.empty(len(p1v), dtype=object)\n",
    "    out[:] = list(zip(p1v, p2v))\n",
    "    return out.reshape(3,3)\n",
    "\n",
    "def expectedUtility(pi1, pi2):\n",
    "    prob_chart = np.array([pi1[a1]*pi2[a2] for a1,a2 in A])\n",
    "    payoff_chart = np.array([getValue(a) for a in A])\n",
    "    u = prob_chart*payoff_chart\n",
    "    return sum(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed = np.ones(NUM_ACTIONS)/NUM_ACTIONS\n",
    "p1v = [getValue(a) for a in A]\n",
    "p2v = [getValue(a) for a in A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi1 = [1/4, 1/2, 1/4]\n",
    "pi2 = [.5, .25, .25]\n",
    "expectedUtility(pi1, pi2) == 1/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.arange(0, 100, 1)\n",
    "f_t = (t + 1) % 2\n",
    "all(f_t[::2]) and not any(f_t[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class Card(IntEnum):\n",
    "    K = 13\n",
    "    Q = 12\n",
    "    J = 11 | 'J'\n",
    "    T = 10 | 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Cards.K: 13>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cards.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
