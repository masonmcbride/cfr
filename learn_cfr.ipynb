{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counterfactual Regret Minimization implementation\n",
    "#Rock Paper Scissors example\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "NUM_ACTIONS = 3\n",
    "NUM_PLAYERS = 2\n",
    "ROCK, PAPER, SCISSORS = range(NUM_ACTIONS)\n",
    "\n",
    "A = np.array(list(itertools.product(range(NUM_ACTIONS), repeat=NUM_PLAYERS))) #all combinations of simul actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, regrets_sum=np.zeros(NUM_ACTIONS)):\n",
    "        self._strategy = np.zeros(NUM_ACTIONS)\n",
    "        self.strategy_sum = np.zeros(NUM_ACTIONS)\n",
    "        self.regrets_sum = regrets_sum\n",
    "        self.number_of_updates = 0\n",
    "    \n",
    "    @property\n",
    "    def strategy(self):\n",
    "        self._strategy = np.maximum(self.regrets_sum, 0)\n",
    "        self._strategy = self._strategy/sum(self._strategy) if sum(self._strategy) > 0 \\\n",
    "                    else np.ones(NUM_ACTIONS)/NUM_ACTIONS\n",
    "        self.strategy_sum += self._strategy\n",
    "        return self._strategy\n",
    "    \n",
    "    def payoff(self, my_action, other_action):\n",
    "        mod_3 = (my_action - other_action) % 3\n",
    "        if mod_3 == 2:\n",
    "            return -1\n",
    "        else:\n",
    "            return mod_3\n",
    "    \n",
    "    def update_regrets(self, other_action, my_payoff):\n",
    "        self.regrets_sum += np.array([self.payoff(a, other_action) - my_payoff for a in range(NUM_ACTIONS)])\n",
    "    \n",
    "    def update_strategy(self, my_action, other_action):\n",
    "        my_payoff = self.payoff(my_action, other_action)\n",
    "        self.update_regrets(other_action, my_payoff)\n",
    "        self.number_of_updates += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.33594638, 0.33138933, 0.33266428]),\n",
       " array([0.33816863, 0.32780438, 0.33402699]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opp_strategy = np.array([.3, .3, .4])\n",
    "me = Player(regrets_sum=np.array([400, 150, 0]))\n",
    "opp = Player()\n",
    "my_optimal, opp_optimal = train(100000)\n",
    "my_optimal, opp_optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opp.strategy_sum / opp.number_of_updates\n",
    "print(expectedUtility([1-.25-.6,.25,.6], mixed))\n",
    "expectedUtility(mixed, [1-.25-.6,.25,.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(strategy):\n",
    "    return np.random.choice(range(NUM_ACTIONS), p=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations):\n",
    "    for _ in range(iterations):\n",
    "        #⟨Get regret-matched mixed-strategy actions⟩\n",
    "        my_action = get_action(me.strategy)\n",
    "        opp_action = get_action(opp.strategy)\n",
    "        me.update_strategy(my_action, opp_action)\n",
    "        opp.update_strategy(opp_action, my_action)\n",
    "    \n",
    "    my_optimal = me.strategy_sum / me.number_of_updates\n",
    "    opp_optimal = opp.strategy_sum / opp.number_of_updates    \n",
    "    return my_optimal, opp_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random things i learned while cfr\n",
    "def getValue(c):\n",
    "    c1, c2 = c\n",
    "    if c1==c2:\n",
    "        return 0\n",
    "    elif c1==0 and c2==2:\n",
    "        return 1\n",
    "    elif c1==1 and c2==0:\n",
    "        return 1\n",
    "    elif c1==2 and c2==1:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "def payoffTable(p1v, p2v):\n",
    "    out = np.empty(len(p1v), dtype=object)\n",
    "    out[:] = list(zip(p1v, p2v))\n",
    "    return out.reshape(3,3)\n",
    "\n",
    "def expectedUtility(pi1, pi2):\n",
    "    prob_chart = np.array([pi1[a1]*pi2[a2] for a1,a2 in A])\n",
    "    payoff_chart = np.array([getValue(a) for a in A])\n",
    "    u = prob_chart*payoff_chart\n",
    "    return sum(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed = np.ones(NUM_ACTIONS)/NUM_ACTIONS\n",
    "p1v = [getValue(a) for a in A]\n",
    "p2v = [getValue(a) for a in A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi1 = [1/4, 1/2, 1/4]\n",
    "pi2 = [.5, .25, .25]\n",
    "expectedUtility(pi1, pi2) == 1/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
